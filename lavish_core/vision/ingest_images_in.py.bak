"""
Lavish 'Wall Street Edition' â€” Image Ingestion â†’ OCR â†’ Parsed Labels
--------------------------------------------------------------------
- Scans data/images_in (recursive)
- OCR with PaddleOCR (if installed) â†’ fallback to Tesseract
- Threaded, cached, resumable, robust logging
- Parses trades via lavish_core.vision.parser_triggers.parse_trade_text
  (fallback: lavish_core.vision.parser.parse_trade_text; else safe regex parser)
- Exports JSONL (+ optional CSV), optionally moves processed files
- Never places trades; this is data prep only
"""

from __future__ import annotations
import os, re, csv, sys, json, time, glob, math, hashlib, logging, argparse, shutil
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

# ----------------------
# Config (defaults)
# ----------------------
IMAGES_IN_DIR      = "data/images_in"
IMAGES_PROCESSED   = "data/images_processed"
CACHE_DIR          = "data/ocr_cache"
DATASET_DIR        = "data/ocr"
JSONL_PATH         = os.path.join(DATASET_DIR, "dataset.jsonl")
CSV_PATH           = os.path.join(DATASET_DIR, "dataset.csv")
OK_DIR             = os.path.join(IMAGES_PROCESSED, "ok")
FAIL_DIR           = os.path.join(IMAGES_PROCESSED, "fail")
IMG_EXTS           = {".png", ".jpg", ".jpeg", ".webp", ".bmp", ".tif", ".tiff"}
LOG_FORMAT         = "%(asctime)s | %(levelname)s | %(message)s"
LOG_DATEFMT        = "%Y-%m-%d %H:%M:%S"

logger = logging.getLogger("LavishIngest")
handler = logging.StreamHandler(sys.stdout)
handler.setFormatter(logging.Formatter(LOG_FORMAT, LOG_DATEFMT))
logger.addHandler(handler)
logger.setLevel(logging.INFO)

# ----------------------
# Parser imports (flex)
# ----------------------
def _get_parser():
    try:
        from lavish_core.vision import parser_triggers as P  # preferred
        if hasattr(P, "parse_trade_text"):
            return P.parse_trade_text
    except Exception:
        pass
    try:
        from lavish_core.vision import parser as P  # fallback to your parser.py
        if hasattr(P, "parse_trade_text"):
            return P.parse_trade_text
    except Exception:
        pass

    # Safe regex parser (very conservative)
    BUY_WORDS  = r"(?:buy|call|calls|long)"
    SELL_WORDS = r"(?:sell|put|puts|short|trim|exit)"
    TICK       = r"\b([A-Z]{1,5})\b"

    def _safe_regex_parser(text: str) -> Dict[str, Any]:
        t = text.upper()
        label = None
        if re.search(BUY_WORDS, t):  label = "BUY"
        if re.search(SELL_WORDS, t): label = "SELL" if label is None else label
        tickers = re.findall(TICK, t)
        ticker  = tickers[0] if tickers else ""
        return {
            "label": label or "",
            "ticker": ticker,
            "action": label or "",
            "option_type": "CALL" if "CALL" in t else ("PUT" if "PUT" in t else ""),
            "expiry": "",
            "strike": "",
            "confidence": 0.40 if label or ticker else 0.10,
            "source_text": text[:4000]
        }

    logger.warning("âš ï¸ Using safe regex parser (parser_triggers/parser missing).")
    return _safe_regex_parser

parse_trade_text = _get_parser()

# ----------------------
# OCR (Paddle â†’ Tesseract)
# ----------------------
def _ocr_paddle(img_bgr_or_gray) -> Optional[str]:
    try:
        from paddleocr import PaddleOCR
        ocr = PaddleOCR(use_angle_cls=True, lang="en", show_log=False)
        result = ocr.ocr(img_bgr_or_gray, cls=True)
        lines: List[str] = []
        for page in result or []:
            for item in page or []:
                try:
                    txt = item[1][0]
                    if txt: lines.append(txt)
                except Exception:
                    pass
        return "\n".join(lines).strip()
    except Exception:
        return None

def _ocr_tesseract(img_path: str) -> Optional[str]:
    try:
        import cv2, pytesseract
        img = cv2.imread(img_path)
        if img is None: return None
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        gray = cv2.fastNlMeansDenoising(gray, None, h=3, templateWindowSize=7, searchWindowSize=21)
        return pytesseract.image_to_string(gray, lang="eng")
    except Exception:
        return None

def _ocr(img_path: str) -> str:
    # If Paddle available and can load file quickly, try it via cv2:
    try:
        import cv2
        bgr = cv2.imread(img_path)
    except Exception:
        bgr = None
    text = None
    if bgr is not None:
        text = _ocr_paddle(bgr)
    if not text:
        text = _ocr_tesseract(img_path)
    return (text or "").strip()

# ----------------------
# Helpers
# ----------------------
def ensure_dirs() -> None:
    for p in [IMAGES_IN_DIR, CACHE_DIR, DATASET_DIR, OK_DIR, FAIL_DIR]:
        os.makedirs(p, exist_ok=True)

def list_images(root: str) -> List[str]:
    hits: List[str] = []
    for ext in IMG_EXTS:
        hits.extend(glob.glob(os.path.join(root, f"**/*{ext}"), recursive=True))
    # keep stable order for reproducibility
    hits = sorted({os.path.normpath(p) for p in hits})
    return hits

def sha1_file(path: str) -> str:
    h = hashlib.sha1()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(1 << 16), b""):
            h.update(chunk)
    return h.hexdigest()

def cache_get(key: str) -> Optional[Dict[str, Any]]:
    fp = os.path.join(CACHE_DIR, f"{key}.json")
    if not os.path.exists(fp):
        return None
    try:
        with open(fp, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def cache_put(key: str, rec: Dict[str, Any]) -> None:
    os.makedirs(CACHE_DIR, exist_ok=True)
    tmp = os.path.join(CACHE_DIR, f"{key}.json.tmp")
    out = os.path.join(CACHE_DIR, f"{key}.json")
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(rec, f, ensure_ascii=False)
    os.replace(tmp, out)

def normalize_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()

# ----------------------
# Core worker
# ----------------------
def process_one(img_path: str, fast: bool=False) -> Dict[str, Any]:
    key = sha1_file(img_path)
    cached = cache_get(key)
    if cached and fast:
        rec = dict(cached)
        rec["from_cache"] = True
        return rec

    t0 = time.time()
    text = _ocr(img_path)
    text_norm = normalize_text(text)

    parsed: Dict[str, Any]
    try:
        parsed = parse_trade_text(text_norm) or {}
    except Exception as e:
        parsed = {"error": f"parse-failed: {e}"}

    rec: Dict[str, Any] = {
        "image_path": img_path,
        "cache_key": key,
        "ocr_text": text_norm,
        "parsed": parsed,
        "duration_sec": round(time.time() - t0, 3),
        "timestamp": datetime.utcnow().isoformat() + "Z",
    }
    cache_put(key, rec)
    rec["from_cache"] = False
    return rec

# ----------------------
# Dataset I/O
# ----------------------
DATASET_FIELDS = [
    "image_path", "label", "ticker", "action", "option_type", "expiry",
    "strike", "confidence", "ocr_text", "cache_key"
]

def to_row(rec: Dict[str, Any]) -> Dict[str, Any]:
    p = rec.get("parsed") or {}
    return {
        "image_path": rec.get("image_path",""),
        "label": (p.get("label") or p.get("action") or "").upper(),
        "ticker": p.get("ticker",""),
        "action": (p.get("action") or "").upper(),
        "option_type": (p.get("option_type") or "").upper(),
        "expiry": p.get("expiry",""),
        "strike": p.get("strike",""),
        "confidence": p.get("confidence",""),
        "ocr_text": rec.get("ocr_text",""),
        "cache_key": rec.get("cache_key",""),
    }

def load_existing_jsonl(path: str) -> Dict[str, Dict[str, Any]]:
    """Return map cache_key â†’ record (for dedupe/resume)."""
    m: Dict[str, Dict[str, Any]] = {}
    if not os.path.exists(path):
        return m
    with open(path, "r", encoding="utf-8") as f:
        for ln in f:
            try:
                obj = json.loads(ln)
                k = obj.get("cache_key")
                if k: m[k] = obj
            except Exception:
                continue
    return m

def append_jsonl(records: List[Dict[str, Any]], path: str) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "a", encoding="utf-8") as f:
        for r in records:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")

def write_csv(records: List[Dict[str, Any]], path: str) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=DATASET_FIELDS)
        w.writeheader()
        for r in records:
            w.writerow(to_row(r))

# ----------------------
# Main CLI
# ----------------------
def main():
    ap = argparse.ArgumentParser(description="Lavish Image Ingestion â†’ OCR â†’ Labels")
    ap.add_argument("--images", default=IMAGES_IN_DIR, help="Root folder with images")
    ap.add_argument("--workers", type=int, default=min(8, os.cpu_count() or 4))
    ap.add_argument("--fast", action="store_true", help="Trust cache for OCR results")
    ap.add_argument("--rebuild", action="store_true", help="Ignore existing JSONL and rebuild")
    ap.add_argument("--write-csv", action="store_true", help="Also write CSV next to JSONL")
    ap.add_argument("--move-ok", action="store_true", help="Move successfully parsed images to processed/ok")
    ap.add_argument("--move-fail", action="store_true", help="Move failed images to processed/fail")
    ap.add_argument("--dry-run", action="store_true", help="Do everything except write/move")
    args = ap.parse_args()

    ensure_dirs()

    imgs = list_images(args.images)
    if not imgs:
        logger.warning(f"No images found in {args.images}. Nothing to do.")
        return

    existing_map: Dict[str, Dict[str, Any]] = {}
    if not args.rebuild and os.path.exists(JSONL_PATH):
        existing_map = load_existing_jsonl(JSONL_PATH)
        logger.info(f"Resume: loaded {len(existing_map)} existing records from {JSONL_PATH}")

    # Filter out images already in dataset (if not rebuilding)
    if existing_map and not args.rebuild:
        keep: List[str] = []
        for p in imgs:
            k = sha1_file(p)
            if k not in existing_map:
                keep.append(p)
        imgs = keep

    logger.info(f"ðŸ–¼  Files to process: {len(imgs)}")
    t0 = time.time()
    new_records: List[Dict[str, Any]] = []
    failures = 0

    def _move(src: str, ok: bool):
        if args.dry_run: return
        try:
            base = os.path.basename(src)
            dst_dir = OK_DIR if ok else FAIL_DIR
            os.makedirs(dst_dir, exist_ok=True)
            dst = os.path.join(dst_dir, base)
            if os.path.exists(dst):
                # make unique
                root, ext = os.path.splitext(base)
                dst = os.path.join(dst_dir, f"{root}_{int(time.time())}{ext}")
            shutil.move(src, dst)
        except Exception as e:
            logger.warning(f"Move failed for {src}: {e}")

    # Process
    with ThreadPoolExecutor(max_workers=args.workers) as ex:
        futs = {ex.submit(process_one, p, args.fast): p for p in imgs}
        done = 0
        for fut in as_completed(futs):
            p = futs[fut]
            try:
                rec = fut.result()
                new_records.append(rec)
                ok = bool(rec.get("parsed", {}).get("label") or rec.get("parsed", {}).get("ticker"))
                if args.move_ok and ok: _move(p, ok=True)
                if args.move_fail and not ok: _move(p, ok=False)
                done += 1
                if done % 20 == 0 or done == len(futs):
                    elapsed = time.time() - t0
                    rps = done / elapsed if elapsed > 0 else 0.0
                    logger.info(f"Progress: {done}/{len(futs)}  avg {rps:.2f} img/s")
            except Exception as e:
                failures += 1
                logger.error(f"FAIL {p}: {e}")

    if args.dry_run:
        logger.info("Dry-run: skipping dataset writes.")
        return

    # Persist JSONL (append if resume)
    if args.rebuild or not os.path.exists(JSONL_PATH):
        # write fresh file
        with open(JSONL_PATH, "w", encoding="utf-8") as f:
            for r in new_records:
                f.write(json.dumps(r, ensure_ascii=False) + "\n")
    else:
        append_jsonl(new_records, JSONL_PATH)

    logger.info(f"Saved JSONL â†’ {JSONL_PATH}  (+{len(new_records)} records)")

    if args.write_csv:
        # Re-load everything to write complete CSV snapshot
        full_map = load_existing_jsonl(JSONL_PATH)
        all_records = list(full_map.values())
        write_csv(all_records, CSV_PATH)
        logger.info(f"Saved CSV â†’ {CSV_PATH}  (rows={len(all_records)})")

    elapsed = time.time() - t0
    logger.info(f"Done. New={len(new_records)}  Failures={failures}  Time={elapsed:.2f}s")

if __name__ == "__main__":
    main()