# lavish_core/news/run_master_collector.py
# Finalized Lavish Master Collector (Fixed 'quiet' arg)
# Auto-discovers news_*.py collectors, runs in parallel, retries, dedupes, caches.

from __future__ import annotations

import os
import re
import sys
import json
import time
import hashlib
import importlib
import logging
import traceback
from dataclasses import dataclass, asdict
from datetime import datetime, timezone, timedelta
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, Dict, List, Optional, Iterable

# ------------------ Logging Setup ------------------
def _make_logger(name: str = "LavishMasterCollector", level: str = "INFO") -> logging.Logger:
    logger = logging.getLogger(name)
    if logger.handlers:
        return logger
    logger.setLevel(getattr(logging, level.upper(), logging.INFO))
    handler = logging.StreamHandler(sys.stdout)
    formatter = logging.Formatter("[%(asctime)s] %(levelname)s: %(message)s", datefmt="%H:%M:%S")
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    return logger

logger = _make_logger(level=os.getenv("LOG_LEVEL", "INFO"))

# ------------------ Paths & Defaults ------------------
ROOT = Path(__file__).resolve().parents[2]
CACHE_DIR = ROOT / "data" / "cache"
CACHE_DIR.mkdir(parents=True, exist_ok=True)

DEFAULT_CONCURRENCY = int(os.getenv("LAVISH_COLLECTOR_CONCURRENCY", "6"))
DEFAULT_TIMEOUT = int(os.getenv("LAVISH_COLLECTOR_TIMEOUT", "12"))
DEFAULT_MAX_ARTICLES = int(os.getenv("LAVISH_COLLECTOR_MAX_ARTICLES", "250"))
DEFAULT_SOURCES = os.getenv("LAVISH_COLLECTOR_SOURCES", "auto")

# ------------------ Helpers ------------------
def now_utc() -> datetime:
    return datetime.now(timezone.utc)

def iso(dt: Optional[datetime] = None) -> str:
    return (dt or now_utc()).replace(microsecond=0).isoformat()

def stable_hash(*parts: str) -> str:
    h = hashlib.sha256()
    for p in parts:
        h.update(p.encode("utf-8", errors="ignore"))
    return h.hexdigest()

def atomic_write(path: Path, data: Dict[str, Any]) -> None:
    tmp = path.with_suffix(".tmp")
    tmp.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
    tmp.replace(path)

# ------------------ Data Models ------------------
@dataclass
class Article:
    source: str
    title: str
    url: str
    ts: str
    summary: Optional[str] = None
    tickers: Optional[List[str]] = None
    raw: Optional[Dict[str, Any]] = None

    def fingerprint(self) -> str:
        return stable_hash(self.source, self.title.strip(), self.url.strip())

@dataclass
class CollectorResult:
    name: str
    ok: bool
    items: List[Article]
    took_ms: int
    error: Optional[str] = None
    retries: int = 0

@dataclass
class Circuit:
    failures: int = 0
    open_until: Optional[datetime] = None

    def allow(self) -> bool:
        return not (self.open_until and now_utc() < self.open_until)

    def record(self, ok: bool, backoff: int = 30) -> None:
        if ok:
            self.failures = 0
            self.open_until = None
        else:
            self.failures += 1
            wait = min(180, backoff * (2 ** max(0, self.failures - 1)))
            self.open_until = now_utc() + timedelta(seconds=wait)

# ------------------ Collector Loader ------------------
def _load_collectors(sources: str) -> List[tuple[str, Any]]:
    pkg = "lavish_core.news"
    pkg_path = Path(__file__).resolve().parent
    wanted = None
    if sources and sources != "auto":
        wanted = {s.strip().lower() for s in sources.split(",")}
    modules = []
    for p in sorted(pkg_path.glob("news_*.py")):
        short = p.stem.replace("news_", "")
        if wanted and short.lower() not in wanted:
            continue
        modname = f"{pkg}.{p.stem}"
        try:
            mod = importlib.import_module(modname)
            modules.append((short, mod))
        except Exception as e:
            logger.error(f"Failed to import {modname}: {e}")
    return modules

# ------------------ Invoke Collector ------------------
def _invoke_collector(short: str, mod: Any, timeout: int, max_items: int) -> CollectorResult:
    t0 = time.perf_counter()
    try:
        if hasattr(mod, "Collector"):
            inst = mod.Collector(timeout=timeout, max_items=max_items, logger=logger)
            items: List[Article] = inst.collect()  # type: ignore
        elif hasattr(mod, "collect"):
            items = mod.collect(timeout=timeout, max_items=max_items, logger=logger)  # type: ignore
        else:
            raise RuntimeError("No Collector class or collect() function found")
        took_ms = int((time.perf_counter() - t0) * 1000)
        return CollectorResult(short, True, items, took_ms)
    except Exception as e:
        took_ms = int((time.perf_counter() - t0) * 1000)
        err = f"{type(e).__name__}: {e}"
        logger.debug(traceback.format_exc())
        return CollectorResult(short, False, [], took_ms, error=err)

# ------------------ Deduplication ------------------
def dedupe(articles: Iterable[Article]) -> List[Article]:
    seen = set()
    out = []
    for a in articles:
        fid = a.fingerprint()
        if fid in seen:
            continue
        seen.add(fid)
        out.append(a)
    out.sort(key=lambda x: (x.ts, x.source, x.title))
    return out

# ------------------ Main Orchestration ------------------
def run_master(
    sources: str = DEFAULT_SOURCES,
    timeout: int = DEFAULT_TIMEOUT,
    max_articles: int = DEFAULT_MAX_ARTICLES,
    concurrency: int = DEFAULT_CONCURRENCY,
    quiet: bool = False,
) -> Dict[str, Any]:
    if quiet:
        logger.setLevel(logging.WARNING)

    start = now_utc()
    logger.info("ðŸš€ Starting Lavish Master Collector.")
    mods = _load_collectors(sources)
    if not mods:
        raise SystemExit("No collectors found (need lavish_core/news/news_*.py)")

    logger.info(f"Found collectors: {', '.join(n for n, _ in mods)}")
    circ = {n: Circuit() for n, _ in mods}
    all_items, results = [], []

    def submit_one(name: str, mod: Any) -> CollectorResult:
        c = circ[name]
        if not c.allow():
            return CollectorResult(name, False, [], 0, error="circuit-open")
        backoff = 2
        for attempt in range(1, 4):
            r = _invoke_collector(name, mod, timeout, max_articles)
            if r.ok and r.items:
                c.record(True)
                return r
            c.record(False, backoff)
            wait = backoff + (0.25 * attempt)
            logger.warning(f"[{name}] retry {attempt} ({r.error}); sleeping {wait:.1f}s")
            time.sleep(wait)
            backoff = min(20, backoff * 2)
        return r

    with ThreadPoolExecutor(max_workers=concurrency) as ex:
        futures = {ex.submit(submit_one, n, m): n for n, m in mods}
        for fut in as_completed(futures):
            res = fut.result()
            results.append(res)
            if res.ok:
                logger.info(f"âœ… {res.name}: {len(res.items)} items in {res.took_ms} ms")
                all_items.extend(res.items)
            else:
                logger.error(f"âŒ {res.name}: {res.error}")

    deduped = dedupe(all_items)
    logger.info(f"ðŸ§¹ Deduped {len(all_items)} â†’ {len(deduped)} unique.")

    ts_tag = now_utc().strftime("%Y%m%d_%H%M%S")
    snapshot_path = CACHE_DIR / f"news_snapshot_{ts_tag}.json"
    master_path = CACHE_DIR / "news_master.json"

    payload = {
        "timestamp": iso(),
        "count": len(deduped),
        "items": [asdict(a) for a in deduped],
        "sources": [
            {"name": r.name, "ok": r.ok, "items": len(r.items), "took_ms": r.took_ms, "error": r.error}
            for r in results
        ],
        "meta": {
            "timeout": timeout,
            "max_articles": max_articles,
            "concurrency": concurrency,
            "root": str(ROOT),
        },
    }

    atomic_write(snapshot_path, payload)
    atomic_write(master_path, payload)
    elapsed = (now_utc() - start).total_seconds()
    logger.info(f"ðŸ’¾ Saved {snapshot_path.name} + news_master.json")
    logger.info(f"ðŸ Master Collector completed in {elapsed:.2f}s.")
    return payload

# ------------------ CLI ------------------
def _parse_argv(argv: List[str]) -> Dict[str, Any]:
    import argparse
    p = argparse.ArgumentParser(description="Lavish Master Collector (auto-discovers news_*.py)")
    p.add_argument("--sources", type=str, default=DEFAULT_SOURCES)
    p.add_argument("--timeout", type=int, default=DEFAULT_TIMEOUT)
    p.add_argument("--max-articles", type=int, default=DEFAULT_MAX_ARTICLES)
    p.add_argument("--concurrency", type=int, default=DEFAULT_CONCURRENCY)
    p.add_argument("--quiet", action="store_true", help="Reduce logging noise")
    args = p.parse_args(argv)
    return vars(args)

def main(argv: Optional[List[str]] = None) -> None:
    args = _parse_argv(argv if argv is not None else sys.argv[1:])
    run_master(**args)

if __name__ == "__main__":
    main()
    # --- Auto Fallback Collector (safe default) ---
try:
    from lavish_core.news.run_master_collector import Article
except Exception:
    # local fallback for testing
    from datetime import datetime
    from dataclasses import dataclass
    from typing import List, Optional, Dict, Any
    import hashlib

    @dataclass
    class Article:
        source: str
        title: str
        url: str
        ts: str
        summary: Optional[str] = None
        tickers: Optional[List[str]] = None
        raw: Optional[Dict[str, Any]] = None

        def fingerprint(self) -> str:
            return hashlib.sha256(f"{self.source}{self.title}{self.url}".encode()).hexdigest()


class Collector:
    """Universal fallback Collector to prevent missing-collector errors."""

    def __init__(self, timeout=10, max_items=10, logger=None):
        self.timeout = timeout
        self.max_items = max_items
        self.logger = logger

    def collect(self):
        """Return a placeholder article so the master collector runs cleanly."""
        if self.logger:
            self.logger.debug(f"[{__name__}] Running fallback collector (no API logic yet).")
        return [
            Article(
                source=__name__.split('.')[-1],
                title="Placeholder article â€” collector not implemented yet",
                url="https://placeholder.example.com",
                ts="2025-10-27T00:00:00Z",
                summary="This is a fallback entry until this collector is customized.",
                tickers=[]
            )
        ]
